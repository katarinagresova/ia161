{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA161_Plagiarism Detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMpi2jh7c3SlS0qZruyiVOf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFgPpCOIaFdu"
      },
      "source": [
        "## Plagiarism Detection\n",
        "\n",
        "### Main and related tasks in plagiarism detection\n",
        "\n",
        "* **Plagiarism detection:** Given a document, identify all  plagiarized sources and boundaries of re-used passages.\n",
        "   - similar to deduplication\n",
        "* **Author identification:** Given a document, identify its author.\n",
        "* **Author profiling:** Given a document, extract information about the author (e.g. gender, age).\n",
        "\n",
        "### External vs. Intrinsic plagiarism detection\n",
        "\n",
        "#### External plagiarism detection\n",
        "\n",
        "Given a set of suspicious documents and a set of source documents the\n",
        "task is to find all text passages in the suspicious documents which have\n",
        "been plagiarized and the corresponding text passages in the source\n",
        "documents.\n",
        "\n",
        "#### Intrinsic plagiarism detection\n",
        "\n",
        "Given a set of suspicious documents the task is to identify all plagiarized\n",
        "text passages, e.g., by detecting writing style breaches. The comparison of\n",
        "a suspicious document with other documents is not allowed in this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzVGtYJ9tIjU"
      },
      "source": [
        "# Task: Select a detection algorithm and implement it in Python\n",
        "\n",
        "- Input: File in a 3-column vertical format (word, lemma, tag)\n",
        "- Output: One plagiarism per line: id TAB detected source id TAB real source id. Evaluation line: precision, recall F1 measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3pRAJqQw3sK",
        "outputId": "2420e0d2-e59a-4c46-82d6-88424c049d06"
      },
      "source": [
        "!wget https://nlp.fi.muni.cz/trac/research/raw-attachment/wiki/en/AdvancedNlpCourse/LanguageResourcesFromWeb/training_data.vert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-27 18:05:12--  https://nlp.fi.muni.cz/trac/research/raw-attachment/wiki/en/AdvancedNlpCourse/LanguageResourcesFromWeb/training_data.vert\n",
            "Resolving nlp.fi.muni.cz (nlp.fi.muni.cz)... 147.251.51.11\n",
            "Connecting to nlp.fi.muni.cz (nlp.fi.muni.cz)|147.251.51.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 Ok\n",
            "Length: 503730 (492K) [application/octet-stream]\n",
            "Saving to: ‘training_data.vert’\n",
            "\n",
            "training_data.vert  100%[===================>] 491.92K   672KB/s    in 0.7s    \n",
            "\n",
            "2021-10-27 18:05:14 (672 KB/s) - ‘training_data.vert’ saved [503730/503730]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sa4Gdn9ShEb",
        "outputId": "13258377-695e-44e0-e717-1e0d4f6a256b"
      },
      "source": [
        "!head -n50 training_data.vert"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<doc author=\"Josef Plch\" id=\"101\" class=\"original\" source=\"101\">\n",
            "<s>\n",
            "Reveň\treveň\tk1gFnSc1\n",
            "kadeřavá\tkadeřavý\tk2eAgFnSc1d1\n",
            "Reveň\treveň\tk1gFnSc1\n",
            "kadeřavá\tkadeřavý\tk2eAgFnSc1d1\n",
            "(\t(\tkIx(\n",
            "<g/>\n",
            "Rheum\tRheum\tk1gInSc1\n",
            "rhabarbarum\trhabarbarum\tk1gInSc1\n",
            "<g/>\n",
            ")\t)\tkIx)\n",
            "je\tbýt\tk5eAaImIp3nS\n",
            "rostlina\trostlina\tk1gFnSc1\n",
            "náležící\tnáležící\tk2eAgFnSc1d1\n",
            "do\tdo\tk7c2\n",
            "rodu\trod\tk1gInSc2\n",
            "reveň\treveň\tk1gFnSc1\n",
            "<g/>\n",
            ",\t,\tkIx,\n",
            "čeledi\tčeleď\tk1gFnSc3\n",
            "rdesnovité\trdesnovitý\tk2eAgFnSc2d1\n",
            "<g/>\n",
            ".\t.\tkIx.\n",
            "</s>\n",
            "<s desamb=\"1\">\n",
            "Řapíky\třapík\tk1gInPc1\n",
            "listů\tlist\tk1gInPc2\n",
            "se\tsebe\tk3xPyFc4\n",
            "užívají\tužívat\tk5eAaImIp3nP\n",
            "jako\tjako\tk8xC,k8xS\n",
            "zelenina\tzelenina\tk1gFnSc1\n",
            "<g/>\n",
            ",\t,\tkIx,\n",
            "známá\tznámý\tk2eAgFnSc1d1\n",
            "pod\tpod\tk7c7\n",
            "jménem\tjméno\tk1gNnSc7\n",
            "rebarbora\trebarbora\tk1gFnSc1\n",
            "(\t(\tkIx(\n",
            "<g/>\n",
            "z\tz\tk7c2\n",
            "latinského\tlatinský\tk2eAgInSc2d1\n",
            "názvu\tnázev\tk1gInSc2\n",
            "<g/>\n",
            ")\t)\tkIx)\n",
            "<g/>\n",
            ".\t.\tkIx.\n",
            "</s>\n",
            "<s desamb=\"1\">\n",
            "Synonyma\tsynonymum\tk1gNnPc1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWztCuYbtM1J"
      },
      "source": [
        "import sys, codecs, re\n",
        "\n",
        "def parse_input(input):\n",
        "  \"\"\"\n",
        "  Parse input vert file into dictionary. On top level, documents are grouped by authors. \n",
        "  Each document is represented by dictionary with metadata\n",
        "    - author, \n",
        "    - unique id, \n",
        "    - class (original or suspicious), \n",
        "    - source_id (The same as unique id for originals. Referencing original unique id for suspicious documents.),\n",
        "    - wordlist (set of words with their counts)\n",
        "    - lemmalist (set of lemmas with their counts)\n",
        "  \"\"\"\n",
        "\n",
        "  header_re = re.compile('<doc author=\"([^\"]+)\" id=\"(\\d+)\" class=\"(plagiarism|original)\" source=\"(\\d+)\"')\n",
        "\n",
        "  # reading all docurment into the memory - okey for small amout\n",
        "  doc_sets = {} # sets of documents, each from one author\n",
        "  doc = {}\n",
        "  word_set = {}\n",
        "  lemma_set = {}\n",
        "  N = 0\n",
        "  with open(input, \"r\") as handle:\n",
        "    for line in handle:\n",
        "        if line.startswith('<doc'):\n",
        "\n",
        "            # structure for info about document\n",
        "            author, id_, class_, source_id = header_re.match(line).groups()\n",
        "            doc = {\n",
        "                'author': author,\n",
        "                'id': id_,\n",
        "                'class': class_,\n",
        "                'source_id': source_id,\n",
        "                'wordlist': {},\n",
        "                'lemmalist': {}\n",
        "            }\n",
        "        elif line.startswith('</doc'):\n",
        "\n",
        "            # adding document to author's set - to original of suspisious documents\n",
        "            if not doc['author'] in doc_sets:\n",
        "                doc_sets[doc['author']] = {'original': [], 'suspicious': []}\n",
        "            if doc['class'] == 'original':\n",
        "                doc_sets[doc['author']]['original'].append(doc)\n",
        "            else:\n",
        "                doc_sets[doc['author']]['suspicious'].append(doc)\n",
        "\n",
        "            N += 1\n",
        "        elif not line.startswith('<'):\n",
        "\n",
        "            # adding info about content of document\n",
        "            word, lemma, tag = line.rstrip().split('\\t')[:3]\n",
        "            doc['wordlist'][word] = doc['wordlist'].get(word, 0) + 1\n",
        "            doc['lemmalist'][lemma] = doc['lemmalist'].get(lemma, 0) + 1\n",
        "\n",
        "            word_set[word] = word_set.get(word, 0) + 1\n",
        "            lemma_set[lemma] = lemma_set.get(lemma, 0) + 1\n",
        "\n",
        "    return doc_sets, lemma_set, N"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tkg4On6B5VU"
      },
      "source": [
        "DOC_SIMILARITY_THRESHOLD = 0.5\n",
        "from scipy import spatial\n",
        "\n",
        "def cosine_similarity_wordlist(doc1, doc2, *kwargs):\n",
        "    \"\"\"\n",
        "    Converting documents into vectors and computing their cosine distance.\n",
        "    Each item of a vector represents one word, value of that item represents\n",
        "    relative counts of a word. \n",
        "    Result is a number betwwen 0 and 1 representing similarity of documents \n",
        "    (1 meand identity).\n",
        "    \"\"\"\n",
        "    vector1, vector2 = [], []\n",
        "    all_words = list(doc1['wordlist'].keys() | doc2['wordlist'].keys())\n",
        "    doc1_len = float(sum(doc1['wordlist'].values()))\n",
        "    doc2_len = float(sum(doc2['wordlist'].values()))\n",
        "    for word in all_words:\n",
        "        vector1.append(doc1['wordlist'].get(word, 0) / doc1_len)\n",
        "        vector2.append(doc2['wordlist'].get(word, 0) / doc2_len)\n",
        "    cosine_similarity = 1.0 - spatial.distance.cosine(vector1, vector2)\n",
        "    return cosine_similarity"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AvujBawHChP"
      },
      "source": [
        "def cosine_similarity_lemmalist(doc1, doc2, *kwargs):\n",
        "    \"\"\"\n",
        "    Converting documents into vectors and computing their cosine distance.\n",
        "    Each item of a vector represents one word, value of that item represents\n",
        "    relative counts of a word. \n",
        "    Result is a number betwwen 0 and 1 representing similarity of documents \n",
        "    (1 meand identity).\n",
        "    \"\"\"\n",
        "    vector1, vector2 = [], []\n",
        "    all_words = list(doc1['lemmalist'].keys() | doc2['lemmalist'].keys())\n",
        "    doc1_len = float(sum(doc1['lemmalist'].values()))\n",
        "    doc2_len = float(sum(doc2['lemmalist'].values()))\n",
        "    for word in all_words:\n",
        "        vector1.append(doc1['lemmalist'].get(word, 0) / doc1_len)\n",
        "        vector2.append(doc2['lemmalist'].get(word, 0) / doc2_len)\n",
        "    cosine_similarity = 1.0 - spatial.distance.cosine(vector1, vector2)\n",
        "    return cosine_similarity"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDnb86OMels"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def tf(word, doc):\n",
        "    N = float(sum(doc['lemmalist'].values()))\n",
        "    return doc['lemmalist'][word]/N\n",
        "\n",
        "def idf(word, lemma_set, N):\n",
        "    try:\n",
        "        word_occurance = lemma_set[word] + 1\n",
        "    except:\n",
        "        word_occurance = 1\n",
        "    return np.log(N/word_occurance)\n",
        "\n",
        "def tf_idf(doc, lemma_set, N):\n",
        "    tf_idf_vec = np.zeros((len(lemma_set),))\n",
        "    for word in doc['lemmalist'].keys():\n",
        "        tf_ = tf(word, doc)\n",
        "        idf_ = idf(word, lemma_set, N)\n",
        "          \n",
        "        value = tf_*idf_\n",
        "        tf_idf_vec[lemma_set[word]] = value \n",
        "    return tf_idf_vec\n",
        "\n",
        "def tfidf_lemma(doc1, doc2, lemma_set, N): \n",
        "\n",
        "    vector1 = tf_idf(doc1, lemma_set, N)\n",
        "    vector2 = tf_idf(doc2, lemma_set, N)\n",
        "    cosine_similarity = 1.0 - spatial.distance.cosine(vector1, vector2)\n",
        "    return cosine_similarity\n",
        "  "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkXDDw7PSazo"
      },
      "source": [
        "for author, doc_set in doc_sets.items():\n",
        "      for doc in doc_set['suspicious']:\n",
        "        tf_idf(doc, lemma_set, N)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF4BUWN9FhfX"
      },
      "source": [
        "def evaluate(doc_sets, lemma_set, N, metric):\n",
        "  #Srovname wordlisty podezrelych dokumentu s originaly ze stejne sady dokumentu.\n",
        "  #Zaroven vyhodnocujeme uspesnost.\n",
        "  stats = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "  for author, doc_set in doc_sets.items():\n",
        "      print('Doc set by %s\\n' % author)\n",
        "      set_stats = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "      for doc in doc_set['suspicious']:\n",
        "          #srovnani se vsemi originaly\n",
        "          most_similar_doc_id = doc['id'] #vychozi stav je dokument je nejpodobnejsi sam sobe\n",
        "          highest_similarity_score = 0.0\n",
        "          plagiarism = False\n",
        "          for orig_doc in doc_set['original']:\n",
        "              similarity_score = metric(doc, orig_doc, lemma_set, N)\n",
        "              if similarity_score >= DOC_SIMILARITY_THRESHOLD \\\n",
        "                      and similarity_score > highest_similarity_score:\n",
        "                  most_similar_doc_id = orig_doc['id']\n",
        "                  highest_similarity_score = similarity_score\n",
        "                  plagiarism = True\n",
        "          print('%s\\t%s\\t%s\\n' % (doc['id'], most_similar_doc_id, doc['source_id']))\n",
        "          #vyhodnoceni\n",
        "          if most_similar_doc_id == doc['source_id']:\n",
        "              if doc['class'] == 'plagiarism':\n",
        "                  set_stats['tp'] += 1\n",
        "              else:\n",
        "                  set_stats['tn'] += 1\n",
        "          else:\n",
        "              if doc['class'] == 'plagiarism':\n",
        "                  set_stats['fp'] += 1\n",
        "              else:\n",
        "                  set_stats['fn'] += 1\n",
        "      #vyhodnoceni\n",
        "      try:\n",
        "          precision = set_stats['tp'] / float(set_stats['tp'] + set_stats['fp'])\n",
        "      except ZeroDivisionError:\n",
        "          precision = 0.0\n",
        "      try:\n",
        "          recall = set_stats['tp'] / float(set_stats['tp'] + set_stats['fn'])\n",
        "      except ZeroDivisionError:\n",
        "          recall = 0.0\n",
        "      try:\n",
        "          f1_measure = 2 * precision * recall / (precision + recall)\n",
        "      except ZeroDivisionError:\n",
        "          f1_measure = 0.0\n",
        "      print('Set precision: %.2f, recall: %.2f, F1: %.2f\\n\\n' %\n",
        "          (precision, recall, f1_measure))\n",
        "      stats['tp'] += set_stats['tp']\n",
        "      stats['fp'] += set_stats['fp']\n",
        "      stats['tn'] += set_stats['tn']\n",
        "      stats['fn'] += set_stats['fn']\n",
        "  try:\n",
        "      precision = stats['tp'] / float(stats['tp'] + stats['fp'])\n",
        "  except ZeroDivisionError:\n",
        "      precision = 0.0\n",
        "  try:\n",
        "      recall = stats['tp'] / float(stats['tp'] + stats['fn'])\n",
        "  except ZeroDivisionError:\n",
        "      recall = 0.0\n",
        "  try:\n",
        "      f1_measure = 2 * precision * recall / (precision + recall)\n",
        "  except ZeroDivisionError:\n",
        "      f1_measure = 0.0\n",
        "  print('Overall precision: %.2f, recall: %.2f, F1: %.2f\\n' %\n",
        "      (precision, recall, f1_measure))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUeEQm6uxRL5",
        "outputId": "bcf91eab-cb4b-4016-92a2-91e200964a70"
      },
      "source": [
        "doc_sets, lemma_set, N = parse_input('training_data.vert')\n",
        "evaluate(doc_sets, lemma_set, N, cosine_similarity_wordlist)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc set by Josef Plch\n",
            "\n",
            "106\t101\t101\n",
            "\n",
            "107\t103\t103\n",
            "\n",
            "108\t104\t104\n",
            "\n",
            "109\t104\t104\n",
            "\n",
            "110\t105\t105\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Vojtěch Škvařil\n",
            "\n",
            "202\t201\t201\n",
            "\n",
            "204\t203\t203\n",
            "\n",
            "206\t205\t205\n",
            "\n",
            "208\t207\t207\n",
            "\n",
            "210\t209\t209\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Nikol Volková\n",
            "\n",
            "306\t301\t301\n",
            "\n",
            "307\t302\t302\n",
            "\n",
            "308\t303\t303\n",
            "\n",
            "309\t304\t304\n",
            "\n",
            "310\t305\t305\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Lucie Kaplanová\n",
            "\n",
            "402\t401\t401\n",
            "\n",
            "404\t403\t403\n",
            "\n",
            "406\t405\t405\n",
            "\n",
            "408\t407\t407\n",
            "\n",
            "410\t409\t409\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Matej Gallo\n",
            "\n",
            "506\t501\t501\n",
            "\n",
            "507\t501\t501\n",
            "\n",
            "508\t502\t502\n",
            "\n",
            "509\t502\t503\n",
            "\n",
            "510\t502\t504\n",
            "\n",
            "Set precision: 0.60, recall: 1.00, F1: 0.75\n",
            "\n",
            "\n",
            "Doc set by Lukas Banic\n",
            "\n",
            "606\t605\t601\n",
            "\n",
            "607\t602\t602\n",
            "\n",
            "609\t605\t604\n",
            "\n",
            "610\t605\t605\n",
            "\n",
            "613\t605\t611\n",
            "\n",
            "Set precision: 0.40, recall: 1.00, F1: 0.57\n",
            "\n",
            "\n",
            "Doc set by 422765\n",
            "\n",
            "702\t701\t701\n",
            "\n",
            "704\t703\t703\n",
            "\n",
            "706\t705\t705\n",
            "\n",
            "708\t707\t707\n",
            "\n",
            "710\t709\t709\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Daniela Ryšavá\n",
            "\n",
            "806\t801\t801\n",
            "\n",
            "807\t802\t802\n",
            "\n",
            "808\t803\t803\n",
            "\n",
            "809\t804\t804\n",
            "\n",
            "810\t805\t805\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Overall precision: 0.88, recall: 1.00, F1: 0.93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAUU0WvTyB0S",
        "outputId": "5ab0f0ae-8365-4155-9447-fbe8b719203f"
      },
      "source": [
        "evaluate(doc_sets, lemma_set, N, cosine_similarity_lemmalist)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc set by Josef Plch\n",
            "\n",
            "106\t101\t101\n",
            "\n",
            "107\t103\t103\n",
            "\n",
            "108\t104\t104\n",
            "\n",
            "109\t104\t104\n",
            "\n",
            "110\t105\t105\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Vojtěch Škvařil\n",
            "\n",
            "202\t201\t201\n",
            "\n",
            "204\t203\t203\n",
            "\n",
            "206\t205\t205\n",
            "\n",
            "208\t207\t207\n",
            "\n",
            "210\t209\t209\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Nikol Volková\n",
            "\n",
            "306\t301\t301\n",
            "\n",
            "307\t303\t302\n",
            "\n",
            "308\t303\t303\n",
            "\n",
            "309\t303\t304\n",
            "\n",
            "310\t301\t305\n",
            "\n",
            "Set precision: 0.40, recall: 1.00, F1: 0.57\n",
            "\n",
            "\n",
            "Doc set by Lucie Kaplanová\n",
            "\n",
            "402\t401\t401\n",
            "\n",
            "404\t403\t403\n",
            "\n",
            "406\t405\t405\n",
            "\n",
            "408\t407\t407\n",
            "\n",
            "410\t409\t409\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Doc set by Matej Gallo\n",
            "\n",
            "506\t501\t501\n",
            "\n",
            "507\t501\t501\n",
            "\n",
            "508\t502\t502\n",
            "\n",
            "509\t502\t503\n",
            "\n",
            "510\t502\t504\n",
            "\n",
            "Set precision: 0.60, recall: 1.00, F1: 0.75\n",
            "\n",
            "\n",
            "Doc set by Lukas Banic\n",
            "\n",
            "606\t605\t601\n",
            "\n",
            "607\t604\t602\n",
            "\n",
            "609\t604\t604\n",
            "\n",
            "610\t605\t605\n",
            "\n",
            "613\t604\t611\n",
            "\n",
            "Set precision: 0.40, recall: 1.00, F1: 0.57\n",
            "\n",
            "\n",
            "Doc set by 422765\n",
            "\n",
            "702\t701\t701\n",
            "\n",
            "704\t703\t703\n",
            "\n",
            "706\t705\t705\n",
            "\n",
            "708\t703\t707\n",
            "\n",
            "710\t703\t709\n",
            "\n",
            "Set precision: 0.60, recall: 1.00, F1: 0.75\n",
            "\n",
            "\n",
            "Doc set by Daniela Ryšavá\n",
            "\n",
            "806\t801\t801\n",
            "\n",
            "807\t802\t802\n",
            "\n",
            "808\t803\t803\n",
            "\n",
            "809\t804\t804\n",
            "\n",
            "810\t805\t805\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Overall precision: 0.75, recall: 1.00, F1: 0.86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4H9I-b6SF3r",
        "outputId": "0cb95916-6cd6-4781-8844-a6dad535e1af"
      },
      "source": [
        "evaluate(doc_sets, lemma_set, N, tfidf_lemma)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc set by Josef Plch\n",
            "\n",
            "106\t101\t101\n",
            "\n",
            "107\t103\t103\n",
            "\n",
            "108\t104\t104\n",
            "\n",
            "109\t102\t104\n",
            "\n",
            "110\t105\t105\n",
            "\n",
            "Set precision: 0.80, recall: 1.00, F1: 0.89\n",
            "\n",
            "\n",
            "Doc set by Vojtěch Škvařil\n",
            "\n",
            "202\t201\t201\n",
            "\n",
            "204\t203\t203\n",
            "\n",
            "206\t205\t205\n",
            "\n",
            "208\t201\t207\n",
            "\n",
            "210\t209\t209\n",
            "\n",
            "Set precision: 0.80, recall: 1.00, F1: 0.89\n",
            "\n",
            "\n",
            "Doc set by Nikol Volková\n",
            "\n",
            "306\t304\t301\n",
            "\n",
            "307\t304\t302\n",
            "\n",
            "308\t303\t303\n",
            "\n",
            "309\t304\t304\n",
            "\n",
            "310\t305\t305\n",
            "\n",
            "Set precision: 0.60, recall: 1.00, F1: 0.75\n",
            "\n",
            "\n",
            "Doc set by Lucie Kaplanová\n",
            "\n",
            "402\t401\t401\n",
            "\n",
            "404\t403\t403\n",
            "\n",
            "406\t405\t405\n",
            "\n",
            "408\t407\t407\n",
            "\n",
            "410\t403\t409\n",
            "\n",
            "Set precision: 0.80, recall: 1.00, F1: 0.89\n",
            "\n",
            "\n",
            "Doc set by Matej Gallo\n",
            "\n",
            "506\t501\t501\n",
            "\n",
            "507\t501\t501\n",
            "\n",
            "508\t502\t502\n",
            "\n",
            "509\t502\t503\n",
            "\n",
            "510\t502\t504\n",
            "\n",
            "Set precision: 0.60, recall: 1.00, F1: 0.75\n",
            "\n",
            "\n",
            "Doc set by Lukas Banic\n",
            "\n",
            "606\t611\t601\n",
            "\n",
            "607\t611\t602\n",
            "\n",
            "609\t611\t604\n",
            "\n",
            "610\t605\t605\n",
            "\n",
            "613\t611\t611\n",
            "\n",
            "Set precision: 0.40, recall: 1.00, F1: 0.57\n",
            "\n",
            "\n",
            "Doc set by 422765\n",
            "\n",
            "702\t707\t701\n",
            "\n",
            "704\t703\t703\n",
            "\n",
            "706\t705\t705\n",
            "\n",
            "708\t707\t707\n",
            "\n",
            "710\t707\t709\n",
            "\n",
            "Set precision: 0.60, recall: 1.00, F1: 0.75\n",
            "\n",
            "\n",
            "Doc set by Daniela Ryšavá\n",
            "\n",
            "806\t801\t801\n",
            "\n",
            "807\t802\t802\n",
            "\n",
            "808\t803\t803\n",
            "\n",
            "809\t804\t804\n",
            "\n",
            "810\t805\t805\n",
            "\n",
            "Set precision: 1.00, recall: 1.00, F1: 1.00\n",
            "\n",
            "\n",
            "Overall precision: 0.70, recall: 1.00, F1: 0.82\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
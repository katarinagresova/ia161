{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA161_Plagiarism Detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPyE+S/zRkmhJXPUn7VkAJA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFgPpCOIaFdu"
      },
      "source": [
        "## Plagiarism Detection\n",
        "\n",
        "### Main and related tasks in plagiarism detection\n",
        "\n",
        "* **Plagiarism detection:** Given a document, identify all  plagiarized sources and boundaries of re-used passages.\n",
        "   - similar to deduplication\n",
        "* **Author identification:** Given a document, identify its author.\n",
        "* **Author profiling:** Given a document, extract information about the author (e.g. gender, age).\n",
        "\n",
        "### External vs. Intrinsic plagiarism detection\n",
        "\n",
        "#### External plagiarism detection\n",
        "\n",
        "Given a set of suspicious documents and a set of source documents the\n",
        "task is to find all text passages in the suspicious documents which have\n",
        "been plagiarized and the corresponding text passages in the source\n",
        "documents.\n",
        "\n",
        "#### Intrinsic plagiarism detection\n",
        "\n",
        "Given a set of suspicious documents the task is to identify all plagiarized\n",
        "text passages, e.g., by detecting writing style breaches. The comparison of\n",
        "a suspicious document with other documents is not allowed in this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzVGtYJ9tIjU"
      },
      "source": [
        "# Task: Select a detection algorithm and implement it in Python\n",
        "\n",
        "- Input: File in a 3-column vertical format (word, lemma, tag)\n",
        "- Output: One plagiarism per line: id TAB detected source id TAB real source id. Evaluation line: precision, recall F1 measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3pRAJqQw3sK",
        "outputId": "2420e0d2-e59a-4c46-82d6-88424c049d06"
      },
      "source": [
        "!wget https://nlp.fi.muni.cz/trac/research/raw-attachment/wiki/en/AdvancedNlpCourse/LanguageResourcesFromWeb/training_data.vert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-27 18:05:12--  https://nlp.fi.muni.cz/trac/research/raw-attachment/wiki/en/AdvancedNlpCourse/LanguageResourcesFromWeb/training_data.vert\n",
            "Resolving nlp.fi.muni.cz (nlp.fi.muni.cz)... 147.251.51.11\n",
            "Connecting to nlp.fi.muni.cz (nlp.fi.muni.cz)|147.251.51.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 Ok\n",
            "Length: 503730 (492K) [application/octet-stream]\n",
            "Saving to: ‘training_data.vert’\n",
            "\n",
            "training_data.vert  100%[===================>] 491.92K   672KB/s    in 0.7s    \n",
            "\n",
            "2021-10-27 18:05:14 (672 KB/s) - ‘training_data.vert’ saved [503730/503730]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWztCuYbtM1J"
      },
      "source": [
        "import sys, codecs, re\n",
        "\n",
        "def parse_input(input):\n",
        "  \"\"\"\n",
        "  Parse input vert file into dictionary. On top level, documents are grouped by authors. \n",
        "  Each document is represented by dictionary with metadata\n",
        "    - author, \n",
        "    - unique id, \n",
        "    - class (original or suspicious), \n",
        "    - source_id (The same as unique id for originals. Referencing original unique id for suspicious documents.),\n",
        "    - wordlist (set of words with their counts)\n",
        "    - lemmalist (set of lemmas with their counts)\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  header_re = re.compile('<doc author=\"([^\"]+)\" id=\"(\\d+)\" class=\"(plagiarism|original)\" source=\"(\\d+)\"')\n",
        "\n",
        "  # reading all docurment into the memory - okey for small amout\n",
        "  doc_sets = {} # sets of documents, each from one author\n",
        "  doc = {}\n",
        "  with open(input, \"r\") as handle:\n",
        "    for line in handle:\n",
        "        if line.startswith('<doc'):\n",
        "\n",
        "            # structure for info about document\n",
        "            author, id_, class_, source_id = header_re.match(line).groups()\n",
        "            doc = {\n",
        "                'author': author,\n",
        "                'id': id_,\n",
        "                'class': class_,\n",
        "                'source_id': source_id,\n",
        "                'wordlist': {},\n",
        "                'lemmalist': {}\n",
        "            }\n",
        "        elif line.startswith('</doc'):\n",
        "\n",
        "            # adding document to author's set - to original of suspisious documents\n",
        "            if not doc['author'] in doc_sets:\n",
        "                doc_sets[doc['author']] = {'original': [], 'suspicious': []}\n",
        "            if doc['class'] == 'original':\n",
        "                doc_sets[doc['author']]['original'].append(doc)\n",
        "            else:\n",
        "                doc_sets[doc['author']]['suspicious'].append(doc)\n",
        "        elif not line.startswith('<'):\n",
        "\n",
        "            # adding info about content of document\n",
        "            word, lemma, tag = line.rstrip().split('\\t')[:3]\n",
        "            doc['wordlist'][word] = doc['wordlist'].get(word, 0) + 1\n",
        "            doc['lemmalist'][tag] = doc['lemmalist'].get(tag, 0) + 1\n",
        "\n",
        "    return doc_sets"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUeEQm6uxRL5"
      },
      "source": [
        "doc_sets = parse_input('training_data.vert')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAUU0WvTyB0S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
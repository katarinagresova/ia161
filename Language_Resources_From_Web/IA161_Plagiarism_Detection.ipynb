{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA161_Plagiarism Detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8fCTx0j4xL0H62RoauLty"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFgPpCOIaFdu"
      },
      "source": [
        "## Plagiarism Detection\n",
        "\n",
        "### Main and related tasks in plagiarism detection\n",
        "\n",
        "* **Plagiarism detection:** Given a document, identify all  plagiarized sources and boundaries of re-used passages.\n",
        "   - similar to deduplication\n",
        "* **Author identification:** Given a document, identify its author.\n",
        "* **Author profiling:** Given a document, extract information about the author (e.g. gender, age).\n",
        "\n",
        "### External vs. Intrinsic plagiarism detection\n",
        "\n",
        "#### External plagiarism detection\n",
        "\n",
        "Given a set of suspicious documents and a set of source documents the\n",
        "task is to find all text passages in the suspicious documents which have\n",
        "been plagiarized and the corresponding text passages in the source\n",
        "documents.\n",
        "\n",
        "#### Intrinsic plagiarism detection\n",
        "\n",
        "Given a set of suspicious documents the task is to identify all plagiarized\n",
        "text passages, e.g., by detecting writing style breaches. The comparison of\n",
        "a suspicious document with other documents is not allowed in this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzVGtYJ9tIjU"
      },
      "source": [
        "# Task: Select a detection algorithm and implement it in Python\n",
        "\n",
        "- Input: File in a 3-column vertical format (word, lemma, tag)\n",
        "- Output: One plagiarism per line: id TAB detected source id TAB real source id. Evaluation line: precision, recall F1 measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3pRAJqQw3sK",
        "outputId": "6772c09a-b7a5-4ae6-d8cf-21c4c15c0b71"
      },
      "source": [
        "!wget https://nlp.fi.muni.cz/trac/research/raw-attachment/wiki/en/AdvancedNlpCourse/LanguageResourcesFromWeb/training_data.vert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-29 09:29:14--  https://nlp.fi.muni.cz/trac/research/raw-attachment/wiki/en/AdvancedNlpCourse/LanguageResourcesFromWeb/training_data.vert\n",
            "Resolving nlp.fi.muni.cz (nlp.fi.muni.cz)... 147.251.51.11\n",
            "Connecting to nlp.fi.muni.cz (nlp.fi.muni.cz)|147.251.51.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 Ok\n",
            "Length: 503730 (492K) [application/octet-stream]\n",
            "Saving to: ‘training_data.vert’\n",
            "\n",
            "training_data.vert  100%[===================>] 491.92K   663KB/s    in 0.7s    \n",
            "\n",
            "2021-10-29 09:29:16 (663 KB/s) - ‘training_data.vert’ saved [503730/503730]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYTkz1JtVo68"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "class PlagiarismDetection:\n",
        "  def __init__(self):\n",
        "    self.metadata = None\n",
        "    self.docs = None\n",
        "\n",
        "    # store computations that might be useful for other methods\n",
        "    self.bag_of_words_docs = None\n",
        "\n",
        "  def parse_input(self, vert_file):\n",
        "    header_re = re.compile('<doc author=\"([^\"]+)\" id=\"(\\d+)\" class=\"(plagiarism|original)\" source=\"(\\d+)\"')\n",
        "    self.metadata = {}\n",
        "    self.docs = {}\n",
        "    current_id = None\n",
        "    doc_list = []\n",
        "\n",
        "    with open(vert_file, \"r\") as handle:\n",
        "      for line in handle:\n",
        "\n",
        "        # start of the document - preparing metadata\n",
        "        if line.startswith('<doc'):\n",
        "\n",
        "          # structure for info about document\n",
        "          author, id_, class_, source_id = header_re.match(line).groups()\n",
        "          doc = {\n",
        "            'author': author,\n",
        "            'id': id_,\n",
        "            'class': class_,\n",
        "            'source_id': source_id,\n",
        "          }\n",
        "          current_id = id_\n",
        "          doc_list = []\n",
        "\n",
        "        # end of the document - storing metadata\n",
        "        elif line.startswith('</doc'):\n",
        "\n",
        "          # adding document to author's set - to original of suspisious documents\n",
        "          if not doc['author'] in self.metadata:\n",
        "              self.metadata[doc['author']] = {'original': [], 'suspicious': []}\n",
        "          if doc['class'] == 'original':\n",
        "              self.metadata[doc['author']]['original'].append(doc)\n",
        "          else:\n",
        "              self.metadata[doc['author']]['suspicious'].append(doc)\n",
        "\n",
        "          self.docs[current_id] = pd.DataFrame(doc_list, columns=['word', 'lemma', 'tag'])\n",
        "\n",
        "        elif not line.startswith('<'):\n",
        "\n",
        "          # storing content of document\n",
        "          word, lemma, tag = line.rstrip().split('\\t')[:3]\n",
        "          doc_list.append([word, lemma, tag])\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu0OfiHJYz1w"
      },
      "source": [
        "detector = PlagiarismDetection()\n",
        "detector.parse_input('training_data.vert')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Nrnlox6RZQTl",
        "outputId": "1d22b3dd-94e5-44f1-eb20-04ff61b78a97"
      },
      "source": [
        "detector.docs['101']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reveň</td>\n",
              "      <td>reveň</td>\n",
              "      <td>k1gFnSc1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kadeřavá</td>\n",
              "      <td>kadeřavý</td>\n",
              "      <td>k2eAgFnSc1d1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reveň</td>\n",
              "      <td>reveň</td>\n",
              "      <td>k1gFnSc1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kadeřavá</td>\n",
              "      <td>kadeřavý</td>\n",
              "      <td>k2eAgFnSc1d1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(</td>\n",
              "      <td>(</td>\n",
              "      <td>kIx(</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>listy</td>\n",
              "      <td>lista</td>\n",
              "      <td>k1gFnPc1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>doporučovány</td>\n",
              "      <td>doporučovat</td>\n",
              "      <td>k5eAaImNgFnP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>ke</td>\n",
              "      <td>k</td>\n",
              "      <td>k7c3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>konzumaci</td>\n",
              "      <td>konzumace</td>\n",
              "      <td>k1gFnSc3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>kIx.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>281 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             word        lemma           tag\n",
              "0           Reveň        reveň      k1gFnSc1\n",
              "1        kadeřavá     kadeřavý  k2eAgFnSc1d1\n",
              "2           Reveň        reveň      k1gFnSc1\n",
              "3        kadeřavá     kadeřavý  k2eAgFnSc1d1\n",
              "4               (            (          kIx(\n",
              "..            ...          ...           ...\n",
              "276         listy        lista      k1gFnPc1\n",
              "277  doporučovány  doporučovat  k5eAaImNgFnP\n",
              "278            ke            k          k7c3\n",
              "279     konzumaci    konzumace      k1gFnSc3\n",
              "280             .            .          kIx.\n",
              "\n",
              "[281 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWztCuYbtM1J"
      },
      "source": [
        "import sys, codecs, re\n",
        "\n",
        "def parse_input(input):\n",
        "  \"\"\"\n",
        "  Parse input vert file into dictionary. On top level, documents are grouped by authors. \n",
        "  Each document is represented by dictionary with metadata\n",
        "    - author, \n",
        "    - unique id, \n",
        "    - class (original or suspicious), \n",
        "    - source_id (The same as unique id for originals. Referencing original unique id for suspicious documents.),\n",
        "    - wordlist (set of words with their counts)\n",
        "    - lemmalist (set of lemmas with their counts)\n",
        "  \"\"\"\n",
        "\n",
        "  header_re = re.compile('<doc author=\"([^\"]+)\" id=\"(\\d+)\" class=\"(plagiarism|original)\" source=\"(\\d+)\"')\n",
        "\n",
        "  # reading all docurment into the memory - okey for small amout\n",
        "  doc_sets = {} # sets of documents, each from one author\n",
        "  doc = {}\n",
        "  word_set = {}\n",
        "  lemma_set = {}\n",
        "  N = 0\n",
        "  doc_lemmas = {}\n",
        "  doc_words = {}\n",
        "  with open(input, \"r\") as handle:\n",
        "    for line in handle:\n",
        "        if line.startswith('<doc'):\n",
        "\n",
        "            # structure for info about document\n",
        "            author, id_, class_, source_id = header_re.match(line).groups()\n",
        "            doc = {\n",
        "                'author': author,\n",
        "                'id': id_,\n",
        "                'class': class_,\n",
        "                'source_id': source_id,\n",
        "                'wordlist': {},\n",
        "                'lemmalist': {}\n",
        "            }\n",
        "\n",
        "            doc_lemmas = {}\n",
        "            doc_words = {}\n",
        "\n",
        "        elif line.startswith('</doc'):\n",
        "\n",
        "            # adding document to author's set - to original of suspisious documents\n",
        "            if not doc['author'] in doc_sets:\n",
        "                doc_sets[doc['author']] = {'original': [], 'suspicious': []}\n",
        "            if doc['class'] == 'original':\n",
        "                doc_sets[doc['author']]['original'].append(doc)\n",
        "            else:\n",
        "                doc_sets[doc['author']]['suspicious'].append(doc)\n",
        "\n",
        "            N += 1\n",
        "            for word in doc_words.keys():\n",
        "                word_set[word] = word_set.get(word, 0) + 1\n",
        "            for lemma in doc_lemmas.keys():\n",
        "                lemma_set[lemma] = lemma_set.get(lemma, 0) + 1\n",
        "        elif not line.startswith('<'):\n",
        "\n",
        "            # adding info about content of document\n",
        "            word, lemma, tag = line.rstrip().split('\\t')[:3]\n",
        "            doc['wordlist'][word] = doc['wordlist'].get(word, 0) + 1\n",
        "            doc['lemmalist'][lemma] = doc['lemmalist'].get(lemma, 0) + 1\n",
        "\n",
        "            doc_words[word] = doc_words.get(word, 0) + 1\n",
        "            doc_lemmas[lemma] = doc_lemmas.get(lemma, 0) + 1\n",
        "\n",
        "    return doc_sets, lemma_set, word_set, N"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tkg4On6B5VU"
      },
      "source": [
        "DOC_SIMILARITY_THRESHOLD = 0.5\n",
        "from scipy import spatial\n",
        "\n",
        "def cosine_similarity(doc1, doc2, word_or_lemma_list, **kwargs):\n",
        "    \"\"\"\n",
        "    Converting documents into vectors and computing their cosine distance.\n",
        "    Each item of a vector represents one word, value of that item represents\n",
        "    relative counts of a word. \n",
        "    Result is a number betwwen 0 and 1 representing similarity of documents \n",
        "    (1 meand identity).\n",
        "    \"\"\"\n",
        "    vector1, vector2 = [], []\n",
        "    all_words = list(doc1[word_or_lemma_list].keys() | doc2[word_or_lemma_list].keys())\n",
        "    doc1_len = float(sum(doc1[word_or_lemma_list].values()))\n",
        "    doc2_len = float(sum(doc2[word_or_lemma_list].values()))\n",
        "    for word in all_words:\n",
        "        vector1.append(doc1[word_or_lemma_list].get(word, 0) / doc1_len)\n",
        "        vector2.append(doc2[word_or_lemma_list].get(word, 0) / doc2_len)\n",
        "    cosine_similarity = 1.0 - spatial.distance.cosine(vector1, vector2)\n",
        "    return cosine_similarity"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDnb86OMels"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def tf(word, doc, word_or_lemma_list):\n",
        "    N = float(sum(doc[word_or_lemma_list].values()))\n",
        "    return doc[word_or_lemma_list][word]/N\n",
        "\n",
        "def idf(word, word_or_lemma_set, N):\n",
        "    try:\n",
        "        word_occurance = word_or_lemma_set[word] + 1\n",
        "    except:\n",
        "        word_occurance = 1\n",
        "    return np.log(N/word_occurance)\n",
        "\n",
        "def tf_idf(doc, word_or_lemma_set, word_or_lemma_list, N):\n",
        "    tf_idf_vec = np.zeros((len(word_or_lemma_set),))\n",
        "    for word in doc[word_or_lemma_list].keys():\n",
        "        tf_ = tf(word, doc, word_or_lemma_list)\n",
        "        idf_ = idf(word, word_or_lemma_set, N)\n",
        "          \n",
        "        value = tf_*idf_\n",
        "        tf_idf_vec[word_or_lemma_set[word]] = value \n",
        "    return tf_idf_vec\n",
        "\n",
        "def tfidf_lemma(doc1, doc2, word_or_lemma_list, word_or_lemma_set, N, **kwargs): \n",
        "\n",
        "    vector1 = tf_idf(doc1, word_or_lemma_set, word_or_lemma_list, N)\n",
        "    vector2 = tf_idf(doc2, word_or_lemma_set, word_or_lemma_list, N)\n",
        "    cosine_similarity = 1.0 - spatial.distance.cosine(vector1, vector2)\n",
        "    return cosine_similarity\n",
        "  "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF4BUWN9FhfX"
      },
      "source": [
        "def evaluate(doc_sets, lemma_set, N, metric, word_or_lemma_list, word_or_lemma_set):\n",
        "  #Srovname wordlisty podezrelych dokumentu s originaly ze stejne sady dokumentu.\n",
        "  #Zaroven vyhodnocujeme uspesnost.\n",
        "  stats = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "  for author, doc_set in doc_sets.items():\n",
        "      #print('Doc set by %s\\n' % author)\n",
        "      set_stats = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "      for doc in doc_set['suspicious']:\n",
        "          #srovnani se vsemi originaly\n",
        "          most_similar_doc_id = doc['id'] #vychozi stav je dokument je nejpodobnejsi sam sobe\n",
        "          highest_similarity_score = 0.0\n",
        "          plagiarism = False\n",
        "          for orig_doc in doc_set['original']:\n",
        "              similarity_score = metric(doc1=doc, doc2=orig_doc, word_or_lemma_list=word_or_lemma_list, word_or_lemma_set=word_or_lemma_set, N=N)\n",
        "              if similarity_score >= DOC_SIMILARITY_THRESHOLD \\\n",
        "                      and similarity_score > highest_similarity_score:\n",
        "                  most_similar_doc_id = orig_doc['id']\n",
        "                  highest_similarity_score = similarity_score\n",
        "                  plagiarism = True\n",
        "          #print('%s\\t%s\\t%s\\n' % (doc['id'], most_similar_doc_id, doc['source_id']))\n",
        "          #vyhodnoceni\n",
        "          if most_similar_doc_id == doc['source_id']:\n",
        "              if doc['class'] == 'plagiarism':\n",
        "                  set_stats['tp'] += 1\n",
        "              else:\n",
        "                  set_stats['tn'] += 1\n",
        "          else:\n",
        "              if doc['class'] == 'plagiarism':\n",
        "                  set_stats['fp'] += 1\n",
        "              else:\n",
        "                  set_stats['fn'] += 1\n",
        "      #vyhodnoceni\n",
        "      try:\n",
        "          precision = set_stats['tp'] / float(set_stats['tp'] + set_stats['fp'])\n",
        "      except ZeroDivisionError:\n",
        "          precision = 0.0\n",
        "      try:\n",
        "          recall = set_stats['tp'] / float(set_stats['tp'] + set_stats['fn'])\n",
        "      except ZeroDivisionError:\n",
        "          recall = 0.0\n",
        "      try:\n",
        "          f1_measure = 2 * precision * recall / (precision + recall)\n",
        "      except ZeroDivisionError:\n",
        "          f1_measure = 0.0\n",
        "      #print('Set precision: %.2f, recall: %.2f, F1: %.2f\\n\\n' %\n",
        "      #    (precision, recall, f1_measure))\n",
        "      stats['tp'] += set_stats['tp']\n",
        "      stats['fp'] += set_stats['fp']\n",
        "      stats['tn'] += set_stats['tn']\n",
        "      stats['fn'] += set_stats['fn']\n",
        "  try:\n",
        "      precision = stats['tp'] / float(stats['tp'] + stats['fp'])\n",
        "  except ZeroDivisionError:\n",
        "      precision = 0.0\n",
        "  try:\n",
        "      recall = stats['tp'] / float(stats['tp'] + stats['fn'])\n",
        "  except ZeroDivisionError:\n",
        "      recall = 0.0\n",
        "  try:\n",
        "      f1_measure = 2 * precision * recall / (precision + recall)\n",
        "  except ZeroDivisionError:\n",
        "      f1_measure = 0.0\n",
        "  print('Overall precision: %.2f, recall: %.2f, F1: %.2f\\n' %\n",
        "      (precision, recall, f1_measure))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUeEQm6uxRL5",
        "outputId": "72a86c17-b512-487d-d568-367df1e90f0e"
      },
      "source": [
        "doc_sets, lemma_set, word_set, N = parse_input('training_data.vert')\n",
        "evaluate(doc_sets=doc_sets, lemma_set=lemma_set, N=N, metric=cosine_similarity, word_or_lemma_list='wordlist', word_or_lemma_set=None)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall precision: 0.88, recall: 1.00, F1: 0.93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAUU0WvTyB0S",
        "outputId": "c931e160-a1c4-45fa-9f59-7b5a96ddb3da"
      },
      "source": [
        "evaluate(doc_sets=doc_sets, lemma_set=lemma_set, N=N, metric=cosine_similarity, word_or_lemma_list='lemmalist', word_or_lemma_set=None)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall precision: 0.88, recall: 1.00, F1: 0.93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4H9I-b6SF3r",
        "outputId": "50096bac-81ac-4715-94eb-73b172ffe596"
      },
      "source": [
        "evaluate(doc_sets=doc_sets, lemma_set=lemma_set, N=N, metric=tfidf_lemma, word_or_lemma_list='lemmalist', word_or_lemma_set=lemma_set)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall precision: 0.65, recall: 1.00, F1: 0.79\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEjVg61I-XoY",
        "outputId": "7aac355c-5230-42ca-9460-1202449c2882"
      },
      "source": [
        "evaluate(doc_sets=doc_sets, lemma_set=lemma_set, N=N, metric=tfidf_lemma, word_or_lemma_list='wordlist', word_or_lemma_set=word_set)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall precision: 0.65, recall: 1.00, F1: 0.79\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7VtlnT1Auiw",
        "outputId": "beca3f93-5fdd-4e67-dd13-02f738ffb8fa"
      },
      "source": [
        "lemma_set"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}